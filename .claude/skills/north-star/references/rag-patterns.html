<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG Implementation Patterns - North Star Architecture</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }
        h1 {
            color: #2c3e50;
            font-size: 2.5em;
            margin-bottom: 20px;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #34495e;
            font-size: 2em;
            margin-top: 40px;
            margin-bottom: 20px;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 8px;
        }
        h3 {
            color: #2c3e50;
            font-size: 1.5em;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        h4 {
            color: #34495e;
            font-size: 1.2em;
            margin-top: 15px;
            margin-bottom: 10px;
        }
        p {
            margin-bottom: 15px;
        }
        ul, ol {
            margin-left: 30px;
            margin-bottom: 15px;
        }
        li {
            margin-bottom: 8px;
        }
        pre {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 5px;
            overflow-x: auto;
            margin-bottom: 20px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        code {
            background: #ecf0f1;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        pre code {
            background: transparent;
            padding: 0;
        }
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        .nav {
            background: #34495e;
            color: white;
            padding: 15px;
            border-radius: 5px;
            margin-bottom: 30px;
        }
        .nav a {
            color: #3498db;
            text-decoration: none;
            margin-right: 15px;
            font-weight: 500;
        }
        .nav a:hover {
            text-decoration: underline;
        }
        .pattern-box {
            background: #fff9e6;
            border-left: 4px solid #f39c12;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
        }
        .requirement {
            background: #e8f8f5;
            border-left: 4px solid #27ae60;
            padding: 15px;
            margin: 10px 0;
            border-radius: 4px;
        }
        .workflow {
            background: #fef5e7;
            border-left: 4px solid #f39c12;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav">
            <a href="../SKILL.html">Back to Main</a>
        </div>

        <h1>RAG Implementation Patterns</h1>

        <p>Detailed patterns for implementing Retrieval-Augmented Generation systems.</p>

        <h2>Table of Contents</h2>
        <ol>
            <li>Data Ingestion Patterns</li>
            <li>Chunking Strategies</li>
            <li>Embedding Approaches</li>
            <li>Retrieval Strategies</li>
            <li>Context Assembly</li>
            <li>Citation Patterns</li>
        </ol>

        <h2>1. Data Ingestion Patterns</h2>

        <h3>Pattern: Batch Processing</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Initial data load, scheduled updates</p>

            <pre>def batch_ingest(source_directory):
    documents = load_documents(source_directory)
    for doc in documents:
        chunks = chunk_document(doc)
        embeddings = embed_chunks(chunks)
        store_in_vector_db(embeddings, chunks, metadata)</pre>

            <h4>Pros</h4>
            <p>Simple, complete control</p>

            <h4>Cons</h4>
            <p>Not real-time</p>
        </div>

        <h3>Pattern: Incremental Updates</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Continuous data sources, real-time updates</p>

            <pre>def incremental_ingest(changed_files):
    for file in changed_files:
        # Remove old version
        delete_from_vector_db(file_id)
        # Add new version
        chunks = chunk_document(file)
        embeddings = embed_chunks(chunks)
        store_in_vector_db(embeddings, chunks, metadata)</pre>

            <h4>Pros</h4>
            <p>Always current</p>

            <h4>Cons</h4>
            <p>More complex, needs change detection</p>
        </div>

        <h3>Pattern: Streaming</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>High-volume, continuous data feeds</p>

            <pre>def streaming_ingest(data_stream):
    for data_batch in stream_batches(data_stream, batch_size=100):
        process_and_store(data_batch)</pre>

            <h4>Pros</h4>
            <p>Handles large volumes</p>

            <h4>Cons</h4>
            <p>Complex error handling</p>
        </div>

        <h2>2. Chunking Strategies</h2>

        <h3>Strategy: Fixed Token Count</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Simple MVP, uniform content</p>

            <pre>chunk_size = 512  # tokens
chunk_overlap = 50  # tokens

def chunk_by_tokens(text):
    tokens = tokenize(text)
    for i in range(0, len(tokens), chunk_size - chunk_overlap):
        yield tokens[i:i + chunk_size]</pre>

            <h4>Pros</h4>
            <p>Predictable chunk sizes, fits embedding limits</p>

            <h4>Cons</h4>
            <p>May split semantically-related content</p>
        </div>

        <h3>Strategy: Semantic Chunking</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Narrative content, better quality needed</p>

            <pre>def chunk_semantically(text):
    paragraphs = split_by_paragraphs(text)
    chunks = []
    current_chunk = []
    current_length = 0

    for para in paragraphs:
        para_length = count_tokens(para)
        if current_length + para_length > max_chunk_size:
            chunks.append(join(current_chunk))
            current_chunk = [para]
            current_length = para_length
        else:
            current_chunk.append(para)
            current_length += para_length

    return chunks</pre>

            <h4>Pros</h4>
            <p>Preserves semantic boundaries</p>

            <h4>Cons</h4>
            <p>Variable chunk sizes</p>
        </div>

        <h3>Strategy: Hierarchical Chunking</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Structured documents, context preservation</p>

            <pre>def hierarchical_chunk(document):
    # Store both small chunks and their parent context
    sections = split_by_sections(document)
    for section in sections:
        # Store section-level chunk
        store_chunk(section, level="section")
        # Store paragraph-level chunks with section reference
        for paragraph in split_paragraphs(section):
            store_chunk(paragraph, level="paragraph", parent=section.id)</pre>

            <h4>Pros</h4>
            <p>Multiple granularities, better context</p>

            <h4>Cons</h4>
            <p>More storage, complex retrieval</p>
        </div>

        <h2>3. Embedding Approaches</h2>

        <h3>Approach: Single Embedding Model</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>MVP, uniform content types</p>

            <pre>from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

def embed_chunks(chunks):
    return model.encode(chunks)</pre>

            <h4>Pros</h4>
            <p>Simple, fast</p>

            <h4>Cons</h4>
            <p>May not be optimal for all content types</p>
        </div>

        <h3>Approach: Domain-Specific Models</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Specialized content (code, medical, legal, etc.)</p>

            <pre>models = {
    'code': SentenceTransformer('sentence-transformers/all-mpnet-base-v2'),
    'general': SentenceTransformer('all-MiniLM-L6-v2')
}

def embed_chunks(chunks, content_type):
    model = models.get(content_type, models['general'])
    return model.encode(chunks)</pre>

            <h4>Pros</h4>
            <p>Better quality for specific domains</p>

            <h4>Cons</h4>
            <p>More complex, multiple models to manage</p>
        </div>

        <h3>Approach: Query + Document Models</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Asymmetric search (short query, long documents)</p>

            <pre>query_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')
doc_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')

def embed_query(query):
    return query_model.encode(query)

def embed_documents(docs):
    return doc_model.encode(docs)</pre>

            <h4>Pros</h4>
            <p>Optimized for query-document matching</p>

            <h4>Cons</h4>
            <p>Slightly more complex</p>
        </div>

        <h2>4. Retrieval Strategies</h2>

        <h3>Strategy: Semantic Search Only</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>MVP, well-defined queries</p>

            <pre>def retrieve(query, top_k=5):
    query_embedding = embed_query(query)
    results = vector_db.search(query_embedding, top_k=top_k)
    return results</pre>

            <h4>Pros</h4>
            <p>Simple, works well for semantic matching</p>

            <h4>Cons</h4>
            <p>May miss exact keyword matches</p>
        </div>

        <h3>Strategy: Hybrid Search (Semantic + Keyword)</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Better recall, mix of semantic and exact matches</p>

            <pre>def hybrid_retrieve(query, top_k=5, semantic_weight=0.7):
    # Semantic search
    semantic_results = semantic_search(query, top_k=top_k*2)
    # Keyword search (BM25)
    keyword_results = keyword_search(query, top_k=top_k*2)
    # Combine and rerank
    combined = merge_and_rerank(
        semantic_results,
        keyword_results,
        semantic_weight=semantic_weight
    )
    return combined[:top_k]</pre>

            <h4>Pros</h4>
            <p>Better recall, combines benefits</p>

            <h4>Cons</h4>
            <p>More complex, requires keyword index</p>
        </div>

        <h3>Strategy: Filtered Retrieval</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Multi-tenant, time-sensitive, categorized data</p>

            <pre>def filtered_retrieve(query, filters, top_k=5):
    query_embedding = embed_query(query)
    results = vector_db.search(
        query_embedding,
        top_k=top_k,
        filters={
            'date': {'$gte': filters['start_date']},
            'source': {'$in': filters['allowed_sources']},
            'user_id': filters['user_id']
        }
    )
    return results</pre>

            <h4>Pros</h4>
            <p>Access control, relevance</p>

            <h4>Cons</h4>
            <p>Requires metadata management</p>
        </div>

        <h3>Strategy: Two-Stage Retrieval</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Large document sets, quality > speed</p>

            <pre>def two_stage_retrieve(query, top_k=5):
    # Stage 1: Fast retrieval of many candidates
    candidates = vector_db.search(query_embedding, top_k=top_k*10)

    # Stage 2: Rerank with cross-encoder or LLM
    reranked = rerank_with_cross_encoder(query, candidates)

    return reranked[:top_k]</pre>

            <h4>Pros</h4>
            <p>Higher quality results</p>

            <h4>Cons</h4>
            <p>Slower, more compute</p>
        </div>

        <h2>5. Context Assembly</h2>

        <h3>Pattern: Simple Concatenation</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>MVP, simple queries</p>

            <pre>def assemble_context(query, retrieved_chunks):
    context = "\n\n".join([chunk['text'] for chunk in retrieved_chunks])
    prompt = f"""Answer the question based on the context below.

Context:
{context}

Question: {query}

Answer:"""
    return prompt</pre>

            <h4>Pros</h4>
            <p>Simple, works well</p>

            <h4>Cons</h4>
            <p>No deduplication, may exceed token limits</p>
        </div>

        <h3>Pattern: Token-Aware Assembly</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Token limits, long documents</p>

            <pre>def assemble_context_token_aware(query, retrieved_chunks, max_tokens=2000):
    context_parts = []
    total_tokens = 0

    for chunk in retrieved_chunks:
        chunk_tokens = count_tokens(chunk['text'])
        if total_tokens + chunk_tokens <= max_tokens:
            context_parts.append(chunk['text'])
            total_tokens += chunk_tokens
        else:
            break

    return build_prompt(query, context_parts)</pre>

            <h4>Pros</h4>
            <p>Respects limits, predictable</p>

            <h4>Cons</h4>
            <p>May cut off relevant context</p>
        </div>

        <h3>Pattern: Hierarchical Context</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Structured documents, better understanding</p>

            <pre>def assemble_hierarchical_context(query, retrieved_chunks):
    # Group chunks by document/section
    grouped = group_by_source(retrieved_chunks)

    context = ""
    for source, chunks in grouped.items():
        context += f"\n\n=== {source} ===\n"
        context += "\n".join([c['text'] for c in chunks])

    return build_prompt(query, context)</pre>

            <h4>Pros</h4>
            <p>Better structure, preserves document context</p>

            <h4>Cons</h4>
            <p>More complex formatting</p>
        </div>

        <h2>6. Citation Patterns</h2>

        <h3>Pattern: Chunk-Level Citations</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Transparency, verification</p>

            <pre>def generate_with_citations(query, retrieved_chunks):
    # Build context with IDs
    context_parts = []
    for i, chunk in enumerate(retrieved_chunks):
        context_parts.append(f"[{i+1}] {chunk['text']}")

    prompt = f"""Answer using the numbered sources. Cite sources as [1], [2], etc.

Context:
{chr(10).join(context_parts)}

Question: {query}
Answer with citations:"""

    response = llm.generate(prompt)

    # Return response + source metadata
    return {
        'answer': response,
        'sources': [chunk['metadata'] for chunk in retrieved_chunks]
    }</pre>

            <h4>Pros</h4>
            <p>Clear attribution</p>

            <h4>Cons</h4>
            <p>LLM must follow citation format</p>
        </div>

        <h3>Pattern: Document-Level Citations</h3>

        <div class="pattern-box">
            <h4>When to use</h4>
            <p>Simpler, document-focused</p>

            <pre>def generate_with_doc_citations(query, retrieved_chunks):
    # Group by document
    docs = group_by_document(retrieved_chunks)

    prompt = build_prompt_with_doc_references(query, docs)
    response = llm.generate(prompt)

    return {
        'answer': response,
        'documents_used': [doc['title'] for doc in docs]
    }</pre>

            <h4>Pros</h4>
            <p>Simpler for users</p>

            <h4>Cons</h4>
            <p>Less precise attribution</p>
        </div>

        <h2>Recommended Pattern for MVP</h2>

        <div class="requirement">
            <p>For 1-2 week MVP, start with:</p>
            <ol>
                <li><strong>Ingestion</strong>: Batch processing</li>
                <li><strong>Chunking</strong>: Fixed token count (512 tokens, 50 overlap)</li>
                <li><strong>Embedding</strong>: Single model (all-MiniLM-L6-v2)</li>
                <li><strong>Retrieval</strong>: Semantic search only (top_k=5)</li>
                <li><strong>Context</strong>: Token-aware assembly</li>
                <li><strong>Citation</strong>: Chunk-level citations</li>
            </ol>
            <p>This provides good quality with minimal complexity. Iterate based on real usage.</p>
        </div>

        <h2>Performance Optimization</h2>

        <h3>Caching</h3>

        <p>Cache embeddings for common queries:</p>

        <pre>from functools import lru_cache

@lru_cache(maxsize=1000)
def embed_query_cached(query: str):
    return embed_query(query)</pre>

        <h3>Batch Processing</h3>

        <p>Process multiple chunks at once:</p>

        <pre># Batch embed instead of one-by-one
embeddings = model.encode(chunks, batch_size=32)</pre>

        <h3>Async Operations</h3>

        <p>Use async for I/O operations:</p>

        <pre>async def retrieve_async(query):
    embedding = await embed_query_async(query)
    results = await vector_db.search_async(embedding)
    return results</pre>

        <h2>Quality Evaluation</h2>

        <p>Measure RAG quality:</p>

        <ol>
            <li><strong>Retrieval Precision</strong>: Are retrieved chunks relevant?</li>
            <li><strong>Retrieval Recall</strong>: Are all relevant chunks retrieved?</li>
            <li><strong>Answer Quality</strong>: Is generated answer accurate?</li>
            <li><strong>Citation Accuracy</strong>: Do citations match usage?</li>
        </ol>

        <p>Simple evaluation:</p>

        <pre>def evaluate_retrieval(query, expected_chunks, retrieved_chunks):
    relevant_retrieved = set(retrieved_chunks) & set(expected_chunks)
    precision = len(relevant_retrieved) / len(retrieved_chunks)
    recall = len(relevant_retrieved) / len(expected_chunks)
    return precision, recall</pre>

        <div class="nav">
            <a href="../SKILL.html">Back to Main</a>
        </div>
    </div>
</body>
</html>
